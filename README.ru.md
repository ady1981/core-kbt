# Core-kbt

## Что такое мини-фреймворк core-kbt
### Мотивация для проекта

Сформулируем текущую общую проблематику и мотивацию для проекта в форме тезисов:
* - **Описание**: LLM обучаются на естественном языке, причем обучаются как понятиям ("X описывается деталями Y"), алгоритмам ("нужно сделать шаги такие, чтобы из X получить Y"), так и обратным задачам: "что такое X по деталям Y", "какие шаги нужно сделать, чтобы из X получить Y". 
  - **Проблема и что можно сделать**: для программирования естественный язык крайне неудобен, поэтому нужен "инструментарий", который будет условно транслировать требования задачи в полу-естественный язык так, чтобы (а) LLM однозначно "понимала" задачу в точности, (б) LLM возвращала ответ в ожидаемом виде.
  - **Что сейчас есть в core-kbt**: 
    - AI-функции, для точной передачи задачи в LLM и получения ответа в заданной гибкой структуре
    - для Obsidian: точное управление генерацией через Obsidian Templater теймплеты
* - **Описание**: Текущие LLM ограничены: (а) в объеме своих вычислений, (б) в размере "памяти" для выполнения своих "алгоритмов", (в) **неустойчивости качества алгоритмов при разных характеристиках входных данных**, (г) **особенностями и артифактами, которые есть в обучаемой выборке**, и как результат, (д) **отсутствие каких либо логических гарантий на результат**. С самого своего рождения и до настоящего момента LLM - это машина, которая что-то принимает на вход и что-то выдает на выводе, причем про значение на выходе известно лишь то, что оно будет максимально полезно (при этом заданном входе). Эти ограничение в основном сейчас неконтролируемы.
  - **Примечание**: Стоит отметить, что указанные выше ограничения LLM являются качественными ограничениями на уровне архитектуры и они не будут решены в новых версиях LLM, основанных на текущей архитектуре. Поэтому обозначенные проблемы нуждаются в разработке принципиальных решений.
  - **Проблема и что можно сделать**: для программирования нужны гарантии. Для любого технического инструмента нужны гарантии. Нужен "инструментарий" с такими свойствами:
    - декомпозиция задачи на максимально мелкие элементарные задачи + вычисление элементарные задач + обратные синтез в конечный ответ. Для синтеза нужна архитектура вычислений и оптимизации более высокого уровня
    - поиск и предоставление LLM необходимого контекста
    - контроль ответов от LLM: пост-тестирование, проверка логической корректности, проверка фактологической корректности
  - **Что сейчас есть в core-kbt**:
    - выявление и описание элементарных задач в виде AI-функции
    - предоставление LLM необходимого контекста через базу знаний `elementary`
  - **Что планируется в core-kbt**:
    - проверка логической корректности
    - проверка фактологической корректности
    - архитектура вычислений на основе агентов
    - методы оптимизации базы `elementary` для задачи через "траектории" ("ktb" - "knowledge base trajectory"), т.е. варианты баз знаний для дальнейшего выбора лучшей "траектории" через процедуру оптимизации
    - интеграция с фреймворками для тестирования и мониторинга (Langfuse, n8n и др.)
* - **Описание**: Для профессионального использования LLM недостаточно просто общаться с чатботом. К тому же, как правило, новые знания нужно как-то сохранять для их дальнейшего использования.
  - **Проблема и что можно сделать**: для специального использования нужны специальные инструменты и IDE.
  - **Что сейчас есть в core-kbt**:
    - для Obsidian: скрипты и теймплеты для интеграции функционала core-kbt с Obsidian и полезные инструменты для обработки базы знаний в Obsidian через core-kbt.


### Концепция проекта

Проект `core-kbt` (kbt — Knowledge Base Trajectory) — это мини-фреймворк на Python для разработки приложений на основе больших языковых моделей (LLM). Его основной принцип — **подход с управляемой базой знаний**, где **ИИ-Агенты** в будущем будут использовать и интеллектуально развивать **Базу Знаний** с контролем версий для выполнения сложных задач.

#### ИИ-Функции

ИИ-Функции — это инструменты, которые ИИ-Агенты используют для взаимодействия с миром и модификации Базы Знаний. Они спроектированы как повторно используемые, модульные и "интеллектуальные" компоненты с четко определенными схемами ввода и вывода.

Существует два способа реализации ИИ-Функции:
1.  **Шаблон Jinja2:** Шаблон запроса (промпта), который может быть выполнен LLM.
2.  **Модуль Python:** Функция Python, которая может содержать произвольную логику, включая вызовы внешних API или другие сложные операции.

Сервер Flask предоставляет эти функции через RESTful API, с динамической компиляцией, обрабатывая авторизацию и диспетчеризацию. Это обеспечивает широкую возможность интеграцию ИИ-Функции с другими системами, например:
    * браузерными плагинами
    * n8n-нодами
    * плагинами для работы с базами знаний, например, с Obsidian
    * другими высоко-уровневыми приложениями на любом языке.

#### База Знаний (elementary)

База Знаний — это структурированное хранилище доменных знаний, с которым работают агенты. Она хранится в виде иерархии файлов YAML, JSON или Turtle, что делает ее одновременно читаемой человеком и обрабатываемой машиной.

Вся БЗ хранится в Git, что обеспечивает надежное версионирование. Каждая ветка может представлять различное состояние знаний, что позволяет проводить эксперименты и проводить анализ работы агентов.

#### ИИ-Агенты

ИИ-Агенты — центральные действующие лица в системе. Их основная роль заключается в выполнении задач путем интеллектуального изменения Базы Знаний. Каждое значимое изменение, внесенное агентом, приводит к новому, качественно иному состоянию Базы Знаний (БЗ), которое версионируется в отдельной ветке Git. Это позволяет создавать и оценивать "траекторию" состояний знаний.
В текущий момент архитектура и реализация агентной подсистемы находится в проработке.

## Начало работы

1. Клонируем репо:
```
git clone https://github.com/ady1981/core-kbt.git
cd core-kbt
```
2. Задаем значения переменных среды в `.env` файле, например для DeepSeek:
```shell
DEVELOPMENT=1
HOST=127.0.0.1
PORT=5001
PYTHONUTF8=1
PYTHONIOENCODING=utf8
OPENAI_BASE_URL=https://api.deepseek.com
OPENAI_MODEL=deepseek-chat
OPENAI_API_KEY=<DEEPSEEK_API_TOKEN>
AI_FUNC_API_TOKEN=<A_SECRET>
```
2. Запускаем сервер AI-функций:
  * через docker:
```shell
./run-gh-docker-image.sh
```
   * через командную строку:
```shell
./runner.sh -s kbt-core/ai_function_server.py
```
3. Допустим мы хотим узнать столицу России. Будем использовать готовую AI-функцию `generate` с [промптом](ai_function_templates/generate/prompt.md.j2) и [JSON-схемой ответа](ai_function_templates/generate/output_schema.yaml).
Вызываем AI-функцию с соответствующими входными параметрами (`target_specification`):
```shell
source .env
curl -X PUT "http://127.0.0.1:5001/ai-func/generate" \
  -H "Api-Token: $AI_FUNC_API_TOKEN" \
  -H "Content-Type: application/json" \
  -H "Accept: application/json" \
  -d "{
  \"target_specification\": \"target_description: What is Capital of Russia?\"
}"
```
Ответ:
```
{
  "result": {
    "items": [
      {
        "item": "Moscow",
        "reasoning": "The target question asks for the capital of Russia. Based on general knowledge, the capital of Russia is Moscow."
      }
    ],
    "other_notes": "The information retrieval strategy specified no external context knowledge, relying solely on general knowledge to answer the factual question about the capital of Russia."
  }
}
```

## Развитие идеи

### Универсальный теймплейт для промпта

Понятно, что центральной темой для эффективного использования LLM является промпт-инжиниринг, т.е. подходы к построению эффективных запросов для получения нужных ответов.
Как известно, LLM всегда решает только одну задачу - generatively-continue-prompt. Можно записать общее представление с выделением различных частей для промпта так:
```yaml
LLM_prompt:  
  prompt_structure_and_notation_self_specification: []  
  target_specification:
  - task_specification  
  - task_description  
  - target_semantic_specification  
  - target_semantic_description    
  information_retrieval_strategy:  
  - context_knowledge_specification:  
    - context_knowledge_topic  
    - context_knowledge_source:  
        - properties  
        - content  
  - knowledge_sources_selection_strategy  
  - context_preparation_strategy  
  - contextual_alignment_strategy  
  - contextual_memory_strategy
  - ...  
  output_generation_strategy:  
  - execution_plan_specification  
  - task_decomposition_specification  
  - knowledge_consolidation_specification  
  - evaluation_metrics  
  - iteration_and_refinement_strategy  
  - examples  
  - safety_and_ethics_specification  
  - post_generation_verification_specification  
  - ...  
  output_specification:  
  - structure_and_formatting_specification  
  - output_constrains_specification  
  - output_content_strategy  
  - ...
  ```

Смысл этих частей промпта более-менее ясен из названия. Дополнительно отметим следующее:
* любая specification - задает спецификацию, т.е. однозначно понятные требования
* любая strategy - задает набор политик, как лучше достичь желаемого результата
* если task_specification задать сложно, то можно задать task_description. В дальнейшем результат генерации с таким промптом можно сравнить с результатов генерации для промпта с заданными соответствующим task_specification
* target_specification - задает спецификацию "смысла" запроса и результата
* information_retrieval_strategy - задает как найти информацию во внутренней базе, для решения запроса
* output_generation_strategy - задает стратегию, как генерировать ответ, чтобы прийти к нужному результату.

Приведем пример для промпта в этом представлении:
```yaml
LLM_prompt:
  target_specification:
  - task_specification: Abstractive summarize
  - target_semantic_specification: Concise  
  information_retrieval_strategy:
  - context_knowledge_specification:
    - context_knowledge_source: |
      {{TO_SUMMARIZE_TEXT}}
  - knowledge_sources_selection_strategy: Use only the provided input text.
  - contextual_alignment_strategy: Ensure summary reflects the core meaning of the input.
  output_generation_strategy:
  - focus_on: the central theme
  - execution_plan_specification: Read input, identify key sentences/concepts, synthesize into a short paragraph.
  - task_decomposition_specification: Single step.
  - knowledge_consolidation_specification: Extract and combine main ideas.
  - evaluation_metrics: Conciseness, Fidelity to source.
  - safety_and_ethics_specification: Maintain factual accuracy.
  - post_generation_verification_specification: Check if summary is significantly shorter than the original.
  output_specification:
  - structure_and_formatting_specification: Plain text paragraph.
  - output_constrains_specification: Maximum 3 sentences.

```
Отметим, что существование эффективного универсального теймплейта для LLM промпта означает, что можно сделать одну условно универсальную AI-функцию, через которую можно задать для LLM любую задачу. Причем настраивать такой LLM промпт можно по некоторой универсальной системе аспектов. Однако, для большего удобства и повышения эффективности LLM промптов для выделенных задач имеет смысл создавать отдельные AI-функции, в которых можно более конкретно определять входные поля и, что еще более важно, в выходной JSON-Schema определять более конкретные выходные поля (в том числе для того, чтобы "заставить" модель "подумать" над генерируемыми значениями).

### Идеи для управления промптами
Следующий вопрос состоит в том, как представлять эти части промптов. Предложим набор идей:
* для максимальной эффективности мы хотим прийти к определенной универсальной системе управления промптом. К такой системе, чтобы для любой задачи нужно было итеративно прийти к максимально эффективному промпту. Для этого все базовые элементы спецификации мы будем представлять в виде block-level Markdown свойств в виде: `[<aspect_name>.<feature_name>:: <feature_value>]` (или просто как `<aspect_name>.<feature_name>: <feature_value>`, если это не создает трудности парсинга).
* в core-kbt структурный формат в `output_specification` уже выбран - это JSON Schema, которая подставляется "под капотом". Поэтому в `output_specification` мы будем определять другие аспекты формата ответа.