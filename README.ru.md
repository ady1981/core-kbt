## Концепция проекта

Проект llm-kbt (llm - large language model, kbt - это аббревиатура от Knowledge Base Trajectory) - это мини-фреймворк для разработки траектории доменных баз знаний, с примерами доменных знаний из области LLM.

Идею проекта можно рассматривать с разных сторон:
* сделать удобные средства разработки для LLM-based приложений
* предлагается использовать подход, при котором оптимизировать промпты для задачи нужно через оптимизацию контекста, который вычисляется из базы знаний задачи
* проект является вариантом развития идеи prompting-as-code.

### Со стороны удобства разработки

Для практической разработки приложения, использующего LLM:
* необходимо иметь четкую модель данных того, что нужно подавать LLM на вход и получать на выходе
* удобные возможности создавать новые AI-функции с предсказуемыми свойствами
* работать с schema-less данными, с фиксацией как JSON-схемы, так и семантической-схемы (онтологии), когда это необходимо
* удобно просматривать и модифицировать данные, которые являются контекстом для LLM-промптов
* кешировать ответы от LLM
* распараллеливать запросы к LLM с контролируемым значением RPS для запросов к провайдерам LLM

### Со стороны представления знаний 

Новые возможности, которые дают технологии GenAI, требуют новых подходов к представлению знаний и методов программирования.
LLM модели пропустили через универсальные алгоритмы большие объемы текстовой информации и следует ожидать, что внутреннее представление LLM модели о мире будет соответствовать тому как люди представляют знания. Самым универсальным и близким к человеческому способом представления знаний является отнологическое представление. Поэтому следует ожидать, что именно отнологическое представление будет мостом между "миром представлений LLM" и человеческим представлением знаний, которое необходимо для эффективного решения практических задач.

Предлагается использовать следующий верхне-уровневый более-менее универсальный подход к решению задачи с помощью LLM: 
  * разработать верхне-уровневую онтологию, которая должна удобна и понятна LLM и человеку: [ontology-kb-classes/_all.yaml](elementary%2Fontology-kb-classes%2F_all.yaml)
  * разработать типичные доменные онтологии, которые можно использовать для конкретных задач (пример: [comparison-kb-classes/_all.yaml](elementary%2Fcomparison-kb-classes%2F_all.yaml))
  * решать задачу в следующей постановке:
    * разделить информацию по задаче на следующие блоки ("<как исследователь проблемы с помощью LLM, я описываю>"): 
      * "<что я знаю>": в виде базы знаний на основе отнологии
      * "<что я хочу узнать>": в виде задачи в сущностях базы знаний
      * "<что я хочу сгенерировать>": в виде задачи в сущностях базы знаний.

Постепенно формулируя задачи в сущностях базы знаний и решая эти задачи мы будем постепенно пополнять базу знаний. До тех пор, пока в базе знаний не появятся сущности, в которых содержатся ответы, для которых эта задача и была создана.

## Для чего полезен фреймворк

* база знаний как YAML/JSON-сущности: быстрый и удобный ввод данных для обработки через AI, удобное редактирование сущностей после и между AI обработкой
* JSON-схемы для YAML/JSON-сущностей недостаточно: для углубления знания для решения задачи нужна последовательная разработка доменной онтологии для этой задачи
* AI-функций для быстрого создания и использования функций для обработки через LLM, со структурированным ответом
* персистентные процессы для удобной реализации логики обработки, планирования обработки, кеширования ответов от внешних сервисов и отладки
* разработка траекторий доменных баз знаний: YAML/JSON-сущности и состояние базы знаний хранится как файлы в git, что делает удобным мутации значений сущностей базы знаний через git-бранчевание для дальнейшей оптимизации и поиска лучшего состояния базы знаний

## Фичи фреймворка

* структурированное представление YAML/JSON-сущностей:
  * каждой сущности соответствует папка
  * значение для поля сущности может погружаться из файла
  * есть API для работы с сущностями: см. [items_db.py](kbt-core/items_db.py)
* AI-функций с структурированными ответами:
  * каждой AI-функции соответствует папка 
  * j2 тейплей для промпта, например [prompt.md.j2](ai_functions/list_best_tasks_for_llm_effectivess/prompt.md.j2)
  * требуемая JSON схема для ответа: [output_schema.yaml](ai_functions/list_best_tasks_for_llm_effectivess/output_schema.yaml)
  * есть API для работы с AI-функций, например `evaluate` в [ai_function.py](kbt-core/ai_function.py)
* персистентные процессы для удобной реализации логики обработки, планирования обработки, кеширования ответов от внешних сервисов и отладки:
  * персистентный процесс имеет input (входные данные для процесса), state (текущее состояние процесса) и `status`: `initial` -> `running` -> (`terminated` | `error`) 
  * процессы могут запускаться асинхронно в заданное количество потоков
  * для каждого процесса создается input файл в JSON с входными данными для этого процесса (в папке [processes/input](processes/input)) и state файл процесса в JSON (в папках [processes/by_status](processes/by_status)), в который процесс может записать своё персистентное состояние, и в который записывается результат процесса, когда процесс переходит в `status=terminated` (папка [processes/by_status/terminated](processes/by_status/terminated))
  * логика процесса описывается в файле модуля процесса в папке: [processes/implementation](processes/implementation)
  * процесс определяет как вычисляется его ID в методе `calc_input_id` 
  * создание процесса описывается в соответствующих скриптах
